{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWnZuU7PVtZF"
      },
      "source": [
        "# KRX-Bench 언어 모델 Instruction Tuning 튜토리얼\n",
        "- Unsloth와 금융 도메인 합성 데이터셋을 활용하여 효율적으로 모델을 학습시키는 튜토리얼입니다.\n",
        "- Unsloth는 적은 컴퓨팅 자원으로도 모델을 효율적으로 빠르게 학습시킬 수 있는 방법을 제공하는 라이브러리입니다.\n",
        "\n",
        "## 튜토리얼 환경 및 세부사항\n",
        "- 모델: [Meta-Llama-3.1-8B](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B)\n",
        "- 데이터셋: [amphora/krx-sample-instructions](https://huggingface.co/datasets/amphora/krx-sample-instructions)\n",
        "- 학습 툴: [Unsloth](https://github.com/unslothai/unsloth)\n",
        "- 학습 방법 : Supervised Fine-tuning with QLoRA(4bit)\n",
        "- 학습 컴퓨팅 환경: Google Colab T4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Z-GYk6lFoud"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjizHH4vVtZG"
      },
      "source": [
        "## 1. Unsloth 설치 및 학습 환경 구축"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4DqZvT6VtZG"
      },
      "outputs": [],
      "source": [
        "# # Unsloth 설치 및 최신 버전 업그레이드\n",
        "# !pip install unsloth\n",
        "# !pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "# !pip install trl bitsandbytes triton xformers peft\n",
        "\n",
        "# # GPU가 softcapping을 지원하는 경우, Flash Attention 2 설치\n",
        "# import torch\n",
        "# if torch.cuda.get_device_capability()[0] >= 8:\n",
        "#     !pip install --no-deps packaging ninja einops \"flash-attn>=2.6.3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNfOabrCVtZH"
      },
      "source": [
        "## 2. 모델 및 데이터셋 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sK96swGiVtZH",
        "outputId": "e8a65102-6071-4f96-9790-74f8c80f749b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "==((====))==  Unsloth 2024.10.7: Fast Qwen2 patching. Transformers = 4.44.2.\n",
            "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.69 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.5.0+cu124. CUDA = 8.6. CUDA Toolkit = 12.4.\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: We fixed a gradient accumulation bug, but it seems like you don't have the latest transformers version!\n",
            "Please update transformers, TRL and unsloth via:\n",
            "`pip install --upgrade --no-cache-dir unsloth git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git`\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048\n",
        "dtype = None # None으로 지정할 경우 해당 컴퓨팅 유닛에 알맞은 dtype으로 저장됩니다. Tesla T4와 V100의 경우에는 Float16, Ampere+ 이상의 경우에는 Bfloat16으로 설정됩니다.\n",
        "load_in_4bit = True # 메모리 사용량을 줄이기 위해서는 4bit 양자화를 사용하실 것을 권장합니다.\n",
        "\n",
        "# 모델 및 토크나이저 선언\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"Qwen/Qwen2-7B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    token = \"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\", # gated model을 사용할 경우 허깅페이스 토큰을 입력해주시길 바라겠습니다.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANbyq_gOFouf",
        "outputId": "5585ed5c-1ed1-4b30-fdb0-e3fcf15a0126"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model\n",
            "model.embed_tokens\n",
            "model.layers\n",
            "model.layers.0\n",
            "model.layers.0.self_attn\n",
            "model.layers.0.self_attn.q_proj\n",
            "model.layers.0.self_attn.k_proj\n",
            "model.layers.0.self_attn.v_proj\n",
            "model.layers.0.self_attn.o_proj\n",
            "model.layers.0.self_attn.rotary_emb\n",
            "model.layers.0.mlp\n",
            "model.layers.0.mlp.gate_proj\n",
            "model.layers.0.mlp.up_proj\n",
            "model.layers.0.mlp.down_proj\n",
            "model.layers.0.mlp.act_fn\n",
            "model.layers.0.input_layernorm\n",
            "model.layers.0.post_attention_layernorm\n",
            "model.layers.1\n",
            "model.layers.1.self_attn\n",
            "model.layers.1.self_attn.q_proj\n",
            "model.layers.1.self_attn.k_proj\n",
            "model.layers.1.self_attn.v_proj\n",
            "model.layers.1.self_attn.o_proj\n",
            "model.layers.1.self_attn.rotary_emb\n",
            "model.layers.1.mlp\n",
            "model.layers.1.mlp.gate_proj\n",
            "model.layers.1.mlp.up_proj\n",
            "model.layers.1.mlp.down_proj\n",
            "model.layers.1.mlp.act_fn\n",
            "model.layers.1.input_layernorm\n",
            "model.layers.1.post_attention_layernorm\n",
            "model.layers.2\n",
            "model.layers.2.self_attn\n",
            "model.layers.2.self_attn.q_proj\n",
            "model.layers.2.self_attn.k_proj\n",
            "model.layers.2.self_attn.v_proj\n",
            "model.layers.2.self_attn.o_proj\n",
            "model.layers.2.self_attn.rotary_emb\n",
            "model.layers.2.mlp\n",
            "model.layers.2.mlp.gate_proj\n",
            "model.layers.2.mlp.up_proj\n",
            "model.layers.2.mlp.down_proj\n",
            "model.layers.2.mlp.act_fn\n",
            "model.layers.2.input_layernorm\n",
            "model.layers.2.post_attention_layernorm\n",
            "model.layers.3\n",
            "model.layers.3.self_attn\n",
            "model.layers.3.self_attn.q_proj\n",
            "model.layers.3.self_attn.k_proj\n",
            "model.layers.3.self_attn.v_proj\n",
            "model.layers.3.self_attn.o_proj\n",
            "model.layers.3.self_attn.rotary_emb\n",
            "model.layers.3.mlp\n",
            "model.layers.3.mlp.gate_proj\n",
            "model.layers.3.mlp.up_proj\n",
            "model.layers.3.mlp.down_proj\n",
            "model.layers.3.mlp.act_fn\n",
            "model.layers.3.input_layernorm\n",
            "model.layers.3.post_attention_layernorm\n",
            "model.layers.4\n",
            "model.layers.4.self_attn\n",
            "model.layers.4.self_attn.q_proj\n",
            "model.layers.4.self_attn.k_proj\n",
            "model.layers.4.self_attn.v_proj\n",
            "model.layers.4.self_attn.o_proj\n",
            "model.layers.4.self_attn.rotary_emb\n",
            "model.layers.4.mlp\n",
            "model.layers.4.mlp.gate_proj\n",
            "model.layers.4.mlp.up_proj\n",
            "model.layers.4.mlp.down_proj\n",
            "model.layers.4.mlp.act_fn\n",
            "model.layers.4.input_layernorm\n",
            "model.layers.4.post_attention_layernorm\n",
            "model.layers.5\n",
            "model.layers.5.self_attn\n",
            "model.layers.5.self_attn.q_proj\n",
            "model.layers.5.self_attn.k_proj\n",
            "model.layers.5.self_attn.v_proj\n",
            "model.layers.5.self_attn.o_proj\n",
            "model.layers.5.self_attn.rotary_emb\n",
            "model.layers.5.mlp\n",
            "model.layers.5.mlp.gate_proj\n",
            "model.layers.5.mlp.up_proj\n",
            "model.layers.5.mlp.down_proj\n",
            "model.layers.5.mlp.act_fn\n",
            "model.layers.5.input_layernorm\n",
            "model.layers.5.post_attention_layernorm\n",
            "model.layers.6\n",
            "model.layers.6.self_attn\n",
            "model.layers.6.self_attn.q_proj\n",
            "model.layers.6.self_attn.k_proj\n",
            "model.layers.6.self_attn.v_proj\n",
            "model.layers.6.self_attn.o_proj\n",
            "model.layers.6.self_attn.rotary_emb\n",
            "model.layers.6.mlp\n",
            "model.layers.6.mlp.gate_proj\n",
            "model.layers.6.mlp.up_proj\n",
            "model.layers.6.mlp.down_proj\n",
            "model.layers.6.mlp.act_fn\n",
            "model.layers.6.input_layernorm\n",
            "model.layers.6.post_attention_layernorm\n",
            "model.layers.7\n",
            "model.layers.7.self_attn\n",
            "model.layers.7.self_attn.q_proj\n",
            "model.layers.7.self_attn.k_proj\n",
            "model.layers.7.self_attn.v_proj\n",
            "model.layers.7.self_attn.o_proj\n",
            "model.layers.7.self_attn.rotary_emb\n",
            "model.layers.7.mlp\n",
            "model.layers.7.mlp.gate_proj\n",
            "model.layers.7.mlp.up_proj\n",
            "model.layers.7.mlp.down_proj\n",
            "model.layers.7.mlp.act_fn\n",
            "model.layers.7.input_layernorm\n",
            "model.layers.7.post_attention_layernorm\n",
            "model.layers.8\n",
            "model.layers.8.self_attn\n",
            "model.layers.8.self_attn.q_proj\n",
            "model.layers.8.self_attn.k_proj\n",
            "model.layers.8.self_attn.v_proj\n",
            "model.layers.8.self_attn.o_proj\n",
            "model.layers.8.self_attn.rotary_emb\n",
            "model.layers.8.mlp\n",
            "model.layers.8.mlp.gate_proj\n",
            "model.layers.8.mlp.up_proj\n",
            "model.layers.8.mlp.down_proj\n",
            "model.layers.8.mlp.act_fn\n",
            "model.layers.8.input_layernorm\n",
            "model.layers.8.post_attention_layernorm\n",
            "model.layers.9\n",
            "model.layers.9.self_attn\n",
            "model.layers.9.self_attn.q_proj\n",
            "model.layers.9.self_attn.k_proj\n",
            "model.layers.9.self_attn.v_proj\n",
            "model.layers.9.self_attn.o_proj\n",
            "model.layers.9.self_attn.rotary_emb\n",
            "model.layers.9.mlp\n",
            "model.layers.9.mlp.gate_proj\n",
            "model.layers.9.mlp.up_proj\n",
            "model.layers.9.mlp.down_proj\n",
            "model.layers.9.mlp.act_fn\n",
            "model.layers.9.input_layernorm\n",
            "model.layers.9.post_attention_layernorm\n",
            "model.layers.10\n",
            "model.layers.10.self_attn\n",
            "model.layers.10.self_attn.q_proj\n",
            "model.layers.10.self_attn.k_proj\n",
            "model.layers.10.self_attn.v_proj\n",
            "model.layers.10.self_attn.o_proj\n",
            "model.layers.10.self_attn.rotary_emb\n",
            "model.layers.10.mlp\n",
            "model.layers.10.mlp.gate_proj\n",
            "model.layers.10.mlp.up_proj\n",
            "model.layers.10.mlp.down_proj\n",
            "model.layers.10.mlp.act_fn\n",
            "model.layers.10.input_layernorm\n",
            "model.layers.10.post_attention_layernorm\n",
            "model.layers.11\n",
            "model.layers.11.self_attn\n",
            "model.layers.11.self_attn.q_proj\n",
            "model.layers.11.self_attn.k_proj\n",
            "model.layers.11.self_attn.v_proj\n",
            "model.layers.11.self_attn.o_proj\n",
            "model.layers.11.self_attn.rotary_emb\n",
            "model.layers.11.mlp\n",
            "model.layers.11.mlp.gate_proj\n",
            "model.layers.11.mlp.up_proj\n",
            "model.layers.11.mlp.down_proj\n",
            "model.layers.11.mlp.act_fn\n",
            "model.layers.11.input_layernorm\n",
            "model.layers.11.post_attention_layernorm\n",
            "model.layers.12\n",
            "model.layers.12.self_attn\n",
            "model.layers.12.self_attn.q_proj\n",
            "model.layers.12.self_attn.k_proj\n",
            "model.layers.12.self_attn.v_proj\n",
            "model.layers.12.self_attn.o_proj\n",
            "model.layers.12.self_attn.rotary_emb\n",
            "model.layers.12.mlp\n",
            "model.layers.12.mlp.gate_proj\n",
            "model.layers.12.mlp.up_proj\n",
            "model.layers.12.mlp.down_proj\n",
            "model.layers.12.mlp.act_fn\n",
            "model.layers.12.input_layernorm\n",
            "model.layers.12.post_attention_layernorm\n",
            "model.layers.13\n",
            "model.layers.13.self_attn\n",
            "model.layers.13.self_attn.q_proj\n",
            "model.layers.13.self_attn.k_proj\n",
            "model.layers.13.self_attn.v_proj\n",
            "model.layers.13.self_attn.o_proj\n",
            "model.layers.13.self_attn.rotary_emb\n",
            "model.layers.13.mlp\n",
            "model.layers.13.mlp.gate_proj\n",
            "model.layers.13.mlp.up_proj\n",
            "model.layers.13.mlp.down_proj\n",
            "model.layers.13.mlp.act_fn\n",
            "model.layers.13.input_layernorm\n",
            "model.layers.13.post_attention_layernorm\n",
            "model.layers.14\n",
            "model.layers.14.self_attn\n",
            "model.layers.14.self_attn.q_proj\n",
            "model.layers.14.self_attn.k_proj\n",
            "model.layers.14.self_attn.v_proj\n",
            "model.layers.14.self_attn.o_proj\n",
            "model.layers.14.self_attn.rotary_emb\n",
            "model.layers.14.mlp\n",
            "model.layers.14.mlp.gate_proj\n",
            "model.layers.14.mlp.up_proj\n",
            "model.layers.14.mlp.down_proj\n",
            "model.layers.14.mlp.act_fn\n",
            "model.layers.14.input_layernorm\n",
            "model.layers.14.post_attention_layernorm\n",
            "model.layers.15\n",
            "model.layers.15.self_attn\n",
            "model.layers.15.self_attn.q_proj\n",
            "model.layers.15.self_attn.k_proj\n",
            "model.layers.15.self_attn.v_proj\n",
            "model.layers.15.self_attn.o_proj\n",
            "model.layers.15.self_attn.rotary_emb\n",
            "model.layers.15.mlp\n",
            "model.layers.15.mlp.gate_proj\n",
            "model.layers.15.mlp.up_proj\n",
            "model.layers.15.mlp.down_proj\n",
            "model.layers.15.mlp.act_fn\n",
            "model.layers.15.input_layernorm\n",
            "model.layers.15.post_attention_layernorm\n",
            "model.layers.16\n",
            "model.layers.16.self_attn\n",
            "model.layers.16.self_attn.q_proj\n",
            "model.layers.16.self_attn.k_proj\n",
            "model.layers.16.self_attn.v_proj\n",
            "model.layers.16.self_attn.o_proj\n",
            "model.layers.16.self_attn.rotary_emb\n",
            "model.layers.16.mlp\n",
            "model.layers.16.mlp.gate_proj\n",
            "model.layers.16.mlp.up_proj\n",
            "model.layers.16.mlp.down_proj\n",
            "model.layers.16.mlp.act_fn\n",
            "model.layers.16.input_layernorm\n",
            "model.layers.16.post_attention_layernorm\n",
            "model.layers.17\n",
            "model.layers.17.self_attn\n",
            "model.layers.17.self_attn.q_proj\n",
            "model.layers.17.self_attn.k_proj\n",
            "model.layers.17.self_attn.v_proj\n",
            "model.layers.17.self_attn.o_proj\n",
            "model.layers.17.self_attn.rotary_emb\n",
            "model.layers.17.mlp\n",
            "model.layers.17.mlp.gate_proj\n",
            "model.layers.17.mlp.up_proj\n",
            "model.layers.17.mlp.down_proj\n",
            "model.layers.17.mlp.act_fn\n",
            "model.layers.17.input_layernorm\n",
            "model.layers.17.post_attention_layernorm\n",
            "model.layers.18\n",
            "model.layers.18.self_attn\n",
            "model.layers.18.self_attn.q_proj\n",
            "model.layers.18.self_attn.k_proj\n",
            "model.layers.18.self_attn.v_proj\n",
            "model.layers.18.self_attn.o_proj\n",
            "model.layers.18.self_attn.rotary_emb\n",
            "model.layers.18.mlp\n",
            "model.layers.18.mlp.gate_proj\n",
            "model.layers.18.mlp.up_proj\n",
            "model.layers.18.mlp.down_proj\n",
            "model.layers.18.mlp.act_fn\n",
            "model.layers.18.input_layernorm\n",
            "model.layers.18.post_attention_layernorm\n",
            "model.layers.19\n",
            "model.layers.19.self_attn\n",
            "model.layers.19.self_attn.q_proj\n",
            "model.layers.19.self_attn.k_proj\n",
            "model.layers.19.self_attn.v_proj\n",
            "model.layers.19.self_attn.o_proj\n",
            "model.layers.19.self_attn.rotary_emb\n",
            "model.layers.19.mlp\n",
            "model.layers.19.mlp.gate_proj\n",
            "model.layers.19.mlp.up_proj\n",
            "model.layers.19.mlp.down_proj\n",
            "model.layers.19.mlp.act_fn\n",
            "model.layers.19.input_layernorm\n",
            "model.layers.19.post_attention_layernorm\n",
            "model.layers.20\n",
            "model.layers.20.self_attn\n",
            "model.layers.20.self_attn.q_proj\n",
            "model.layers.20.self_attn.k_proj\n",
            "model.layers.20.self_attn.v_proj\n",
            "model.layers.20.self_attn.o_proj\n",
            "model.layers.20.self_attn.rotary_emb\n",
            "model.layers.20.mlp\n",
            "model.layers.20.mlp.gate_proj\n",
            "model.layers.20.mlp.up_proj\n",
            "model.layers.20.mlp.down_proj\n",
            "model.layers.20.mlp.act_fn\n",
            "model.layers.20.input_layernorm\n",
            "model.layers.20.post_attention_layernorm\n",
            "model.layers.21\n",
            "model.layers.21.self_attn\n",
            "model.layers.21.self_attn.q_proj\n",
            "model.layers.21.self_attn.k_proj\n",
            "model.layers.21.self_attn.v_proj\n",
            "model.layers.21.self_attn.o_proj\n",
            "model.layers.21.self_attn.rotary_emb\n",
            "model.layers.21.mlp\n",
            "model.layers.21.mlp.gate_proj\n",
            "model.layers.21.mlp.up_proj\n",
            "model.layers.21.mlp.down_proj\n",
            "model.layers.21.mlp.act_fn\n",
            "model.layers.21.input_layernorm\n",
            "model.layers.21.post_attention_layernorm\n",
            "model.layers.22\n",
            "model.layers.22.self_attn\n",
            "model.layers.22.self_attn.q_proj\n",
            "model.layers.22.self_attn.k_proj\n",
            "model.layers.22.self_attn.v_proj\n",
            "model.layers.22.self_attn.o_proj\n",
            "model.layers.22.self_attn.rotary_emb\n",
            "model.layers.22.mlp\n",
            "model.layers.22.mlp.gate_proj\n",
            "model.layers.22.mlp.up_proj\n",
            "model.layers.22.mlp.down_proj\n",
            "model.layers.22.mlp.act_fn\n",
            "model.layers.22.input_layernorm\n",
            "model.layers.22.post_attention_layernorm\n",
            "model.layers.23\n",
            "model.layers.23.self_attn\n",
            "model.layers.23.self_attn.q_proj\n",
            "model.layers.23.self_attn.k_proj\n",
            "model.layers.23.self_attn.v_proj\n",
            "model.layers.23.self_attn.o_proj\n",
            "model.layers.23.self_attn.rotary_emb\n",
            "model.layers.23.mlp\n",
            "model.layers.23.mlp.gate_proj\n",
            "model.layers.23.mlp.up_proj\n",
            "model.layers.23.mlp.down_proj\n",
            "model.layers.23.mlp.act_fn\n",
            "model.layers.23.input_layernorm\n",
            "model.layers.23.post_attention_layernorm\n",
            "model.layers.24\n",
            "model.layers.24.self_attn\n",
            "model.layers.24.self_attn.q_proj\n",
            "model.layers.24.self_attn.k_proj\n",
            "model.layers.24.self_attn.v_proj\n",
            "model.layers.24.self_attn.o_proj\n",
            "model.layers.24.self_attn.rotary_emb\n",
            "model.layers.24.mlp\n",
            "model.layers.24.mlp.gate_proj\n",
            "model.layers.24.mlp.up_proj\n",
            "model.layers.24.mlp.down_proj\n",
            "model.layers.24.mlp.act_fn\n",
            "model.layers.24.input_layernorm\n",
            "model.layers.24.post_attention_layernorm\n",
            "model.layers.25\n",
            "model.layers.25.self_attn\n",
            "model.layers.25.self_attn.q_proj\n",
            "model.layers.25.self_attn.k_proj\n",
            "model.layers.25.self_attn.v_proj\n",
            "model.layers.25.self_attn.o_proj\n",
            "model.layers.25.self_attn.rotary_emb\n",
            "model.layers.25.mlp\n",
            "model.layers.25.mlp.gate_proj\n",
            "model.layers.25.mlp.up_proj\n",
            "model.layers.25.mlp.down_proj\n",
            "model.layers.25.mlp.act_fn\n",
            "model.layers.25.input_layernorm\n",
            "model.layers.25.post_attention_layernorm\n",
            "model.layers.26\n",
            "model.layers.26.self_attn\n",
            "model.layers.26.self_attn.q_proj\n",
            "model.layers.26.self_attn.k_proj\n",
            "model.layers.26.self_attn.v_proj\n",
            "model.layers.26.self_attn.o_proj\n",
            "model.layers.26.self_attn.rotary_emb\n",
            "model.layers.26.mlp\n",
            "model.layers.26.mlp.gate_proj\n",
            "model.layers.26.mlp.up_proj\n",
            "model.layers.26.mlp.down_proj\n",
            "model.layers.26.mlp.act_fn\n",
            "model.layers.26.input_layernorm\n",
            "model.layers.26.post_attention_layernorm\n",
            "model.layers.27\n",
            "model.layers.27.self_attn\n",
            "model.layers.27.self_attn.q_proj\n",
            "model.layers.27.self_attn.k_proj\n",
            "model.layers.27.self_attn.v_proj\n",
            "model.layers.27.self_attn.o_proj\n",
            "model.layers.27.self_attn.rotary_emb\n",
            "model.layers.27.mlp\n",
            "model.layers.27.mlp.gate_proj\n",
            "model.layers.27.mlp.up_proj\n",
            "model.layers.27.mlp.down_proj\n",
            "model.layers.27.mlp.act_fn\n",
            "model.layers.27.input_layernorm\n",
            "model.layers.27.post_attention_layernorm\n",
            "model.norm\n",
            "lm_head\n"
          ]
        }
      ],
      "source": [
        "def print_submodules(model, prefix=''):\n",
        "    for name, submodule in model.named_children():\n",
        "        full_name = f\"{prefix}.{name}\" if prefix else name\n",
        "        print(full_name)\n",
        "        print_submodules(submodule, full_name)\n",
        "\n",
        "# Run the function\n",
        "print_submodules(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2mlY9pMVtZH",
        "outputId": "3309c1dd-ee7d-47de-ddca-0cb3bba20e19"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Not an error, but Unsloth cannot patch MLP layers with our manual autograd engine since either LoRA adapters\n",
            "are not enabled or a bias term (like in Qwen) is used.\n",
            "Unsloth 2024.10.7 patched 28 layers with 0 QKV layers, 28 O layers and 0 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "# LoRA Adapter 선언\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 256, # 0을 넘는 숫자를 선택하세요. 8, 16, 32, 64, 128이 추천됩니다.\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], # target module도 적절하게 조정할 수 있습니다.\n",
        "    lora_alpha = 64,\n",
        "    lora_dropout = 0, # 어떤 값이든 사용될 수 있지만, 0으로 최적화되어 있습니다.\n",
        "    bias = \"none\", # 어떤 값이든 사용될 수 있지만, \"none\"으로 최적화되어 있습니다.\n",
        "    use_gradient_checkpointing = \"unsloth\", # 매우 긴 context에 대해 True 또는 \"unsloth\"를 사용하십시오.\n",
        "    random_state = 42,\n",
        "    use_rslora = True,\n",
        "    loftq_config = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loDn97x9VtZH"
      },
      "source": [
        "## 3. 데이터셋 전처리\n",
        "데이터셋은 한국어 금융 합성 데이터셋인 `amphora/krx-sample-instruction`를 사용하였고, 데이터셋 템플릿으로는 Alpaca prompt에서 input을 제거한 형태의 프롬프트 템플릿을 사용하였습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEDXXvLTVtZH"
      },
      "outputs": [],
      "source": [
        "prompt_format = \"\"\"\n",
        "지금부터 당신은 금융과 재무 회계 전문가로서 다음 규칙에 따라 문제를 해결합니다.\n",
        "\n",
        "[학습 데이터의 구조]\n",
        "1. 문제 열: 질문이 포함되어 있습니다.\n",
        "2. 해설 열: 문제에 대한 설명이 포함되어 있습니다.\n",
        "3. 정답 열: 객관식 문제의 올바른 정답 번호가 포함되어 있거나, 주관식 문제의 경우 비어 있습니다.\n",
        "4. 문제 유형 열: 문제 유형이 '객관식' 또는 '주관식'으로 명시되어 있습니다.\n",
        "\n",
        "[당신의 역할]\n",
        "1. 객관식 문제:\n",
        "- 입력된 질문을 분석합니다.\n",
        "- 학습 데이터에 기반하여 해당 질문의 정답 번호만 출력합니다.\n",
        "- 불필요한 텍스트, 설명, 해석은 출력하지 않습니다.\n",
        "\n",
        "2. 주관식 문제:\n",
        "-입력된 질문을 분석합니다.\n",
        "-학습 데이터에 기반하여 해당 질문의 해설 내용을 정답으로 출력합니다.\n",
        "-해설 내용을 정확히 출력하며 불필요한 텍스트는 포함하지 않습니다.\n",
        "\n",
        "--------------------------------------------------------------------\n",
        "아래는 객관식 문제 입력과 출력 예시입니다.\n",
        "\n",
        "##질문 예시 1##\n",
        "최고가격제와 최저가격제에 대한 설명 중 옳은 것을 <보기>에서 고르면?\n",
        "<보기>\n",
        "ㄱ. 최고가격제는 가격을 시장 균형 가격 이하로 통제하는 제도다.\n",
        "ㄴ. 최고가격제의 대표적인 예는 농산물 가격 지지정책이다.\n",
        "ㄷ. 시장에서 최저가격제를 시행하면 초과 공급이 발생한다.\n",
        "ㄹ. 노동 시장에서 최저가격제는 생산요소시장에서 소비자를 보호하기 위한 정책이다.\n",
        "① ㄱ,ㄴ ② ㄱ,ㄷ ③ ㄴ,ㄷ ④ ㄴ,ㄹ ⑤ ㄷ,ㄹ\n",
        "##출력 예시 1##\n",
        "②\n",
        "\n",
        "##질문 예시 2##\n",
        "아래 신문기사의 (A)에 들어갈 적합한 용어는?\n",
        "\"통계청의 <가계금융복지조사>에 따르면 최근 3년간 3040세대 가구의 부동산 보유액 증가율이 매년 두 자릿수를 기록하며 금융 자산인 저축액의 증가율을 2배 이상 앞섰다. 이들 가구의 부동산 투자의 재원은 어디서 온 것일까? 통계청의 <가계동향조사>로부터 재원의 상당 부분이 금융 부채로 충당됐음을 알 수 있다. 재무 건전성 지표가 지난해엔 116.8로 악화됐다. 재무 건전성 지표가 100을 웃돌면 저축액으로 금융 부채를 전부 상환하지 못하는 상태를 의미한다. 여전히 (A)를(를) 기대하는 사람들이 많다는 뜻이다.\"\n",
        "① 캐시카우 ② 엥겔 법칙 ③ 핀볼 효과 ④ 파레토 법칙 ⑤ 레버리지 효과\n",
        "##출력 예시 2##\n",
        "⑤\n",
        "\n",
        "##질문 예시 3##\n",
        "(가), (나)에 들어갈 알파벳을 순서대로 올바르게 고른 것은?\n",
        "● 오랜 경제 침체 속에 소비자 물가 하락세가 점점 심화하면서 이른바 (가)의 공포가 확산되고 있다.\n",
        "● 더블 딥(double dip)이란 불황에서 벗어나 회복 기미를 보이던 경기가 다시 침체에 빠지는 것으로, 두 번의 경기침체를 겪는다는 점에서 (나)자형 침체로도 불린다.\n",
        "① D - M ② D - W ③ X - M ④ R - M ⑤ R - W\n",
        "##출력 예시 3##\n",
        "②\n",
        "\n",
        "##질문 예시 4##\n",
        "기업이 차등의결권, 포이즌 필, 황금낙하산 제도를 활용하는 가장 큰 목적은 무엇인가?\n",
        "① 분산투자 ② 경영권 방어 ③ 물류망 확충 ④ 노동생산성 향상 ⑤ 인수·합병(M&A) 촉진\n",
        "##출력 예시 4##\n",
        "②\n",
        "\n",
        "--------------------------------------------------------------------\n",
        "아래는 주관식 문제 입력과 출력 예시입니다.\n",
        "\n",
        "##질문 예시 1##\n",
        "R의 공포와 S의 공포란?\n",
        "##출력 예시 1##\n",
        "보통 신문 기사나 뉴스에서 언급하는 R의 공포는 ‘Recession(경기침체)’을 의미한다. 통상 시장이 ‘기술적 경기침체’로 간주하는 상황은 GDP 증감률이 두 분기 연속 마이너스 성장할 때이다. 저성장과 고물가 늪에 빠지는 경우 ‘S’의 공포에 빠졌다고 표현한다. 보통 S를 스태그플레이션(Stagflation)을 의미한다. 스태그플레이션이란 경기가 침체해 수요가 감소함에도 오히려 물가가 오르는 현상이다. 경기침체인 스태그네이션(Stagnation)과 물가 상승을 의미하는 인플레이션(Inflation)의 합성어이다.\n",
        "\n",
        "##질문 예시 2##\n",
        "잠재GDP가 무엇인지 설명하시오.\n",
        "##출력 예시 2##\n",
        "잠재GDP란 한 나라의 경제가 인플레이션을 일으키지 않고 생산할 수 있는 최대 생산량을 의미하며,경제의 생산능력을 나타내는 지표이다. 잠재GDP는 노동력,자본,기술 등의 생산요소가 최대한 활용할 때 달성할 수 있는 생산량으로,경제의 구조적 특성과 경제 정책 등에 영향을 받는다. 잠재GDP는 실제GDP와 비교해경제정책의 수립과 평가에도 중요한 역할을 한다.예를 들어정부가 경제정책을 수립할 때는 잠재GDP를 고려하여 적절한 정책을 수립해야 하며,경제의 성장률이 잠재GDP를 초과할 경우 인플레이션이 발생할 가능성이 커지므로,경제정책을 조정하여 인플레이션을 예방해야 한다.\n",
        "\n",
        "--------------------------------------------------------------------\n",
        "반드시 아래 규칙을 따라야 합니다.\n",
        "1. 객관식 문제는 정답 번호만 출력합니다.\n",
        "2. 주관식 문제는 해설 내용을 정답으로 출력합니다.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"prompt\"]\n",
        "    outputs = examples[\"response\"]\n",
        "    texts = []\n",
        "    for instruction, output in zip(instructions, outputs):\n",
        "        text = prompt_format.format(instruction, output) + EOS_TOKEN # 마지막에 eos token을 추가해줌으로써 모델이 출력을 끝마칠 수 있게 만들어 줍니다.\n",
        "        texts.append(text)\n",
        "    return { \"formatted_text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"amphora/krx-sample-instructions\", split = \"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yboq7rrnFouh",
        "outputId": "fddcd9e8-597d-4d4f-ae9a-0b9f19c2a006"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'prompt': '음악산업의 디지털화가 진행됨에 따라, 전통적인 음반 중심의 수익 모델에서 디지털 음악 서비스 중심으로 변화했음을 설명하는데, 이러한 변화가 이루어진 주된 원인은 무엇이며, 이에 따라 업계가 어떻게 대응하고 있는지 구체적인 예를 들어 설명하시오.',\n",
              " 'response': '음악산업의 디지털화 주된 원인은 인터넷과 мобиль 서비스의 발달로 인해 소비자들이 음악을 접근하고 소비하는 방식이 변화했기 때문입니다. 디지털 플랫폼의 등장으로 음원 스트리밍 서비스(예: 멜론, 스포티파이 등)가 보편화되면서, 소비자들은 과거의 음반 구매 대신 구독 기반의 서비스를 통해 음악에 쉽게 접근하게 되었습니다.\\n\\n이에 따라 음악업계는 다양한 대응 전략을 마련하고 있습니다. 예를 들어, 아티스트들은 음원 스트리밍 수익을 최적화하기 위해 SNS와 유튜브를 활용한 마케팅을 강화하고 있으며, 라이브 공연 및 팬미팅과 같은 오프라인 이벤트를 확대하여 추가 수익을 창출하고 있습니다. 또한, 일부 음악 레이블은 아티스트와의 협업을 통해 독점 콘텐츠를 제작하고, 메타버스와 같은 새로운 플랫폼을 이용해 가상 공연을 개최하는 등의 혁신적인 접근법을 시도하고 있습니다.',\n",
              " '__index_level_0__': 0,\n",
              " 'formatted_text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n음악산업의 디지털화가 진행됨에 따라, 전통적인 음반 중심의 수익 모델에서 디지털 음악 서비스 중심으로 변화했음을 설명하는데, 이러한 변화가 이루어진 주된 원인은 무엇이며, 이에 따라 업계가 어떻게 대응하고 있는지 구체적인 예를 들어 설명하시오.\\n\\n### Response:\\n음악산업의 디지털화 주된 원인은 인터넷과 мобиль 서비스의 발달로 인해 소비자들이 음악을 접근하고 소비하는 방식이 변화했기 때문입니다. 디지털 플랫폼의 등장으로 음원 스트리밍 서비스(예: 멜론, 스포티파이 등)가 보편화되면서, 소비자들은 과거의 음반 구매 대신 구독 기반의 서비스를 통해 음악에 쉽게 접근하게 되었습니다.\\n\\n이에 따라 음악업계는 다양한 대응 전략을 마련하고 있습니다. 예를 들어, 아티스트들은 음원 스트리밍 수익을 최적화하기 위해 SNS와 유튜브를 활용한 마케팅을 강화하고 있으며, 라이브 공연 및 팬미팅과 같은 오프라인 이벤트를 확대하여 추가 수익을 창출하고 있습니다. 또한, 일부 음악 레이블은 아티스트와의 협업을 통해 독점 콘텐츠를 제작하고, 메타버스와 같은 새로운 플랫폼을 이용해 가상 공연을 개최하는 등의 혁신적인 접근법을 시도하고 있습니다.<|im_end|>'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNT3z9nNVtZH"
      },
      "source": [
        "## 4. 모델 학습\n",
        "\n",
        "full train을 위해서는 `num_train_epochs=1`로 설정하고, `max_steps`를 설정하지 않으면 됩니다.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7K5p11fVtZH",
        "outputId": "6c55656b-b4de-45d4-f742-eaa2919ae88c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"formatted_text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # True로 설정하면 짧은 텍스트 데이터에 대해서는 더 빠른 학습 속도로를 보여줍니다.\n",
        "    args = TrainingArguments( # TrainingArguments는 자신의 학습 환경과 기호에 따라 적절하게 설정하면 됩니다.\n",
        "        per_device_train_batch_size = 8,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 1,\n",
        "        # max_steps = 60,\n",
        "        learning_rate = 1e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 42,\n",
        "        output_dir = \"./outputs\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97c2KpwfbNmP",
        "outputId": "9f3463d4-e880-406e-bf08-f6aedba44a1c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 25,951 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 32 | Total steps = 811\n",
            " \"-____-\"     Number of trainable parameters = 161,480,704\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**** Unsloth: Please use our fixed gradient_accumulation_steps by updating transformers, TRL and Unsloth!\n",
            "`pip install --upgrade --no-cache-dir unsloth git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git`\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='85' max='811' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 85/811 25:25 < 3:42:23, 0.05 it/s, Epoch 0.10/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.241400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.110600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.090700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.033500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.065700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.044100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.070100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.015000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZl4bf7ZVtZI"
      },
      "source": [
        "## 5. 모델 추론"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHa2tCFgVtZI"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    prompt_format.format(\n",
        "        \"수요와 공급의 원리에 대해서 설명해줘.\", # instruction\n",
        "        \"\", # output 생성을 위해 빈 칸으로 설정\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ9S7MWeVtZI"
      },
      "source": [
        "## 6. 모델 저장 및 업로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DsyXPKqVtZI"
      },
      "outputs": [],
      "source": [
        "# LoRA Adapter 저장\n",
        "model.save_pretrained(\"lora_model\")\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "\n",
        "# Merged model 저장 및 업로드\n",
        "model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "model.push_to_hub_merged(\"mssongit/qwen2-7b-instruct-v0\", tokenizer, save_method = \"merged_16bit\", token = \"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\") # 개인 huggingface token을 사용하여 업로드할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAJJR1YVLYdg"
      },
      "source": [
        "## 참고자료\n",
        "\n",
        "- [Unsloth GitHub](https://github.com/unslothai/unsloth)\n",
        "- [Unsloth Docs](https://docs.unsloth.ai/)\n",
        "- [Unsloth Meta-Llama-3.1-8B Finetuning Tutorial](https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing)\n",
        "- [Huggingface PEFT](https://github.com/huggingface/peft)\n",
        "- [Huggingface TRL](https://github.com/huggingface/trl)\n",
        "- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)\n",
        "- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)\n",
        "- [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/abs/2307.08691)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LlRCGpIFoui"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXqlMGAyFoui"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ir",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}