{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWnZuU7PVtZF"
      },
      "source": [
        "# KRX-Bench ì–¸ì–´ ëª¨ë¸ Instruction Tuning íŠœí† ë¦¬ì–¼\n",
        "- Unslothì™€ ê¸ˆìœµ ë„ë©”ì¸ í•©ì„± ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ íš¨ìœ¨ì ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” íŠœí† ë¦¬ì–¼ì…ë‹ˆë‹¤.\n",
        "- UnslothëŠ” ì ì€ ì»´í“¨íŒ… ìì›ìœ¼ë¡œë„ ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ ë¹ ë¥´ê²Œ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì œê³µí•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\n",
        "\n",
        "## íŠœí† ë¦¬ì–¼ í™˜ê²½ ë° ì„¸ë¶€ì‚¬í•­\n",
        "- ëª¨ë¸: [Meta-Llama-3.1-8B](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B)\n",
        "- ë°ì´í„°ì…‹: [amphora/krx-sample-instructions](https://huggingface.co/datasets/amphora/krx-sample-instructions)\n",
        "- í•™ìŠµ íˆ´: [Unsloth](https://github.com/unslothai/unsloth)\n",
        "- í•™ìŠµ ë°©ë²• : Supervised Fine-tuning with QLoRA(4bit)\n",
        "- í•™ìŠµ ì»´í“¨íŒ… í™˜ê²½: Google Colab T4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Z-GYk6lFoud"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjizHH4vVtZG"
      },
      "source": [
        "## 1. Unsloth ì„¤ì¹˜ ë° í•™ìŠµ í™˜ê²½ êµ¬ì¶•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4DqZvT6VtZG"
      },
      "outputs": [],
      "source": [
        "# # Unsloth ì„¤ì¹˜ ë° ìµœì‹  ë²„ì „ ì—…ê·¸ë ˆì´ë“œ\n",
        "# !pip install unsloth\n",
        "# !pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "# !pip install trl bitsandbytes triton xformers peft\n",
        "\n",
        "# # GPUê°€ softcappingì„ ì§€ì›í•˜ëŠ” ê²½ìš°, Flash Attention 2 ì„¤ì¹˜\n",
        "# import torch\n",
        "# if torch.cuda.get_device_capability()[0] >= 8:\n",
        "#     !pip install --no-deps packaging ninja einops \"flash-attn>=2.6.3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNfOabrCVtZH"
      },
      "source": [
        "## 2. ëª¨ë¸ ë° ë°ì´í„°ì…‹ ë¡œë“œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sK96swGiVtZH",
        "outputId": "e8a65102-6071-4f96-9790-74f8c80f749b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "==((====))==  Unsloth 2024.10.7: Fast Qwen2 patching. Transformers = 4.44.2.\n",
            "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.69 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.5.0+cu124. CUDA = 8.6. CUDA Toolkit = 12.4.\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: We fixed a gradient accumulation bug, but it seems like you don't have the latest transformers version!\n",
            "Please update transformers, TRL and unsloth via:\n",
            "`pip install --upgrade --no-cache-dir unsloth git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git`\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048\n",
        "dtype = None # Noneìœ¼ë¡œ ì§€ì •í•  ê²½ìš° í•´ë‹¹ ì»´í“¨íŒ… ìœ ë‹›ì— ì•Œë§ì€ dtypeìœ¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤. Tesla T4ì™€ V100ì˜ ê²½ìš°ì—ëŠ” Float16, Ampere+ ì´ìƒì˜ ê²½ìš°ì—ëŠ” Bfloat16ìœ¼ë¡œ ì„¤ì •ë©ë‹ˆë‹¤.\n",
        "load_in_4bit = True # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ì„œëŠ” 4bit ì–‘ìí™”ë¥¼ ì‚¬ìš©í•˜ì‹¤ ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n",
        "\n",
        "# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì„ ì–¸\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"Qwen/Qwen2-7B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    token = \"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\", # gated modelì„ ì‚¬ìš©í•  ê²½ìš° í—ˆê¹…í˜ì´ìŠ¤ í† í°ì„ ì…ë ¥í•´ì£¼ì‹œê¸¸ ë°”ë¼ê² ìŠµë‹ˆë‹¤.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANbyq_gOFouf",
        "outputId": "5585ed5c-1ed1-4b30-fdb0-e3fcf15a0126"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model\n",
            "model.embed_tokens\n",
            "model.layers\n",
            "model.layers.0\n",
            "model.layers.0.self_attn\n",
            "model.layers.0.self_attn.q_proj\n",
            "model.layers.0.self_attn.k_proj\n",
            "model.layers.0.self_attn.v_proj\n",
            "model.layers.0.self_attn.o_proj\n",
            "model.layers.0.self_attn.rotary_emb\n",
            "model.layers.0.mlp\n",
            "model.layers.0.mlp.gate_proj\n",
            "model.layers.0.mlp.up_proj\n",
            "model.layers.0.mlp.down_proj\n",
            "model.layers.0.mlp.act_fn\n",
            "model.layers.0.input_layernorm\n",
            "model.layers.0.post_attention_layernorm\n",
            "model.layers.1\n",
            "model.layers.1.self_attn\n",
            "model.layers.1.self_attn.q_proj\n",
            "model.layers.1.self_attn.k_proj\n",
            "model.layers.1.self_attn.v_proj\n",
            "model.layers.1.self_attn.o_proj\n",
            "model.layers.1.self_attn.rotary_emb\n",
            "model.layers.1.mlp\n",
            "model.layers.1.mlp.gate_proj\n",
            "model.layers.1.mlp.up_proj\n",
            "model.layers.1.mlp.down_proj\n",
            "model.layers.1.mlp.act_fn\n",
            "model.layers.1.input_layernorm\n",
            "model.layers.1.post_attention_layernorm\n",
            "model.layers.2\n",
            "model.layers.2.self_attn\n",
            "model.layers.2.self_attn.q_proj\n",
            "model.layers.2.self_attn.k_proj\n",
            "model.layers.2.self_attn.v_proj\n",
            "model.layers.2.self_attn.o_proj\n",
            "model.layers.2.self_attn.rotary_emb\n",
            "model.layers.2.mlp\n",
            "model.layers.2.mlp.gate_proj\n",
            "model.layers.2.mlp.up_proj\n",
            "model.layers.2.mlp.down_proj\n",
            "model.layers.2.mlp.act_fn\n",
            "model.layers.2.input_layernorm\n",
            "model.layers.2.post_attention_layernorm\n",
            "model.layers.3\n",
            "model.layers.3.self_attn\n",
            "model.layers.3.self_attn.q_proj\n",
            "model.layers.3.self_attn.k_proj\n",
            "model.layers.3.self_attn.v_proj\n",
            "model.layers.3.self_attn.o_proj\n",
            "model.layers.3.self_attn.rotary_emb\n",
            "model.layers.3.mlp\n",
            "model.layers.3.mlp.gate_proj\n",
            "model.layers.3.mlp.up_proj\n",
            "model.layers.3.mlp.down_proj\n",
            "model.layers.3.mlp.act_fn\n",
            "model.layers.3.input_layernorm\n",
            "model.layers.3.post_attention_layernorm\n",
            "model.layers.4\n",
            "model.layers.4.self_attn\n",
            "model.layers.4.self_attn.q_proj\n",
            "model.layers.4.self_attn.k_proj\n",
            "model.layers.4.self_attn.v_proj\n",
            "model.layers.4.self_attn.o_proj\n",
            "model.layers.4.self_attn.rotary_emb\n",
            "model.layers.4.mlp\n",
            "model.layers.4.mlp.gate_proj\n",
            "model.layers.4.mlp.up_proj\n",
            "model.layers.4.mlp.down_proj\n",
            "model.layers.4.mlp.act_fn\n",
            "model.layers.4.input_layernorm\n",
            "model.layers.4.post_attention_layernorm\n",
            "model.layers.5\n",
            "model.layers.5.self_attn\n",
            "model.layers.5.self_attn.q_proj\n",
            "model.layers.5.self_attn.k_proj\n",
            "model.layers.5.self_attn.v_proj\n",
            "model.layers.5.self_attn.o_proj\n",
            "model.layers.5.self_attn.rotary_emb\n",
            "model.layers.5.mlp\n",
            "model.layers.5.mlp.gate_proj\n",
            "model.layers.5.mlp.up_proj\n",
            "model.layers.5.mlp.down_proj\n",
            "model.layers.5.mlp.act_fn\n",
            "model.layers.5.input_layernorm\n",
            "model.layers.5.post_attention_layernorm\n",
            "model.layers.6\n",
            "model.layers.6.self_attn\n",
            "model.layers.6.self_attn.q_proj\n",
            "model.layers.6.self_attn.k_proj\n",
            "model.layers.6.self_attn.v_proj\n",
            "model.layers.6.self_attn.o_proj\n",
            "model.layers.6.self_attn.rotary_emb\n",
            "model.layers.6.mlp\n",
            "model.layers.6.mlp.gate_proj\n",
            "model.layers.6.mlp.up_proj\n",
            "model.layers.6.mlp.down_proj\n",
            "model.layers.6.mlp.act_fn\n",
            "model.layers.6.input_layernorm\n",
            "model.layers.6.post_attention_layernorm\n",
            "model.layers.7\n",
            "model.layers.7.self_attn\n",
            "model.layers.7.self_attn.q_proj\n",
            "model.layers.7.self_attn.k_proj\n",
            "model.layers.7.self_attn.v_proj\n",
            "model.layers.7.self_attn.o_proj\n",
            "model.layers.7.self_attn.rotary_emb\n",
            "model.layers.7.mlp\n",
            "model.layers.7.mlp.gate_proj\n",
            "model.layers.7.mlp.up_proj\n",
            "model.layers.7.mlp.down_proj\n",
            "model.layers.7.mlp.act_fn\n",
            "model.layers.7.input_layernorm\n",
            "model.layers.7.post_attention_layernorm\n",
            "model.layers.8\n",
            "model.layers.8.self_attn\n",
            "model.layers.8.self_attn.q_proj\n",
            "model.layers.8.self_attn.k_proj\n",
            "model.layers.8.self_attn.v_proj\n",
            "model.layers.8.self_attn.o_proj\n",
            "model.layers.8.self_attn.rotary_emb\n",
            "model.layers.8.mlp\n",
            "model.layers.8.mlp.gate_proj\n",
            "model.layers.8.mlp.up_proj\n",
            "model.layers.8.mlp.down_proj\n",
            "model.layers.8.mlp.act_fn\n",
            "model.layers.8.input_layernorm\n",
            "model.layers.8.post_attention_layernorm\n",
            "model.layers.9\n",
            "model.layers.9.self_attn\n",
            "model.layers.9.self_attn.q_proj\n",
            "model.layers.9.self_attn.k_proj\n",
            "model.layers.9.self_attn.v_proj\n",
            "model.layers.9.self_attn.o_proj\n",
            "model.layers.9.self_attn.rotary_emb\n",
            "model.layers.9.mlp\n",
            "model.layers.9.mlp.gate_proj\n",
            "model.layers.9.mlp.up_proj\n",
            "model.layers.9.mlp.down_proj\n",
            "model.layers.9.mlp.act_fn\n",
            "model.layers.9.input_layernorm\n",
            "model.layers.9.post_attention_layernorm\n",
            "model.layers.10\n",
            "model.layers.10.self_attn\n",
            "model.layers.10.self_attn.q_proj\n",
            "model.layers.10.self_attn.k_proj\n",
            "model.layers.10.self_attn.v_proj\n",
            "model.layers.10.self_attn.o_proj\n",
            "model.layers.10.self_attn.rotary_emb\n",
            "model.layers.10.mlp\n",
            "model.layers.10.mlp.gate_proj\n",
            "model.layers.10.mlp.up_proj\n",
            "model.layers.10.mlp.down_proj\n",
            "model.layers.10.mlp.act_fn\n",
            "model.layers.10.input_layernorm\n",
            "model.layers.10.post_attention_layernorm\n",
            "model.layers.11\n",
            "model.layers.11.self_attn\n",
            "model.layers.11.self_attn.q_proj\n",
            "model.layers.11.self_attn.k_proj\n",
            "model.layers.11.self_attn.v_proj\n",
            "model.layers.11.self_attn.o_proj\n",
            "model.layers.11.self_attn.rotary_emb\n",
            "model.layers.11.mlp\n",
            "model.layers.11.mlp.gate_proj\n",
            "model.layers.11.mlp.up_proj\n",
            "model.layers.11.mlp.down_proj\n",
            "model.layers.11.mlp.act_fn\n",
            "model.layers.11.input_layernorm\n",
            "model.layers.11.post_attention_layernorm\n",
            "model.layers.12\n",
            "model.layers.12.self_attn\n",
            "model.layers.12.self_attn.q_proj\n",
            "model.layers.12.self_attn.k_proj\n",
            "model.layers.12.self_attn.v_proj\n",
            "model.layers.12.self_attn.o_proj\n",
            "model.layers.12.self_attn.rotary_emb\n",
            "model.layers.12.mlp\n",
            "model.layers.12.mlp.gate_proj\n",
            "model.layers.12.mlp.up_proj\n",
            "model.layers.12.mlp.down_proj\n",
            "model.layers.12.mlp.act_fn\n",
            "model.layers.12.input_layernorm\n",
            "model.layers.12.post_attention_layernorm\n",
            "model.layers.13\n",
            "model.layers.13.self_attn\n",
            "model.layers.13.self_attn.q_proj\n",
            "model.layers.13.self_attn.k_proj\n",
            "model.layers.13.self_attn.v_proj\n",
            "model.layers.13.self_attn.o_proj\n",
            "model.layers.13.self_attn.rotary_emb\n",
            "model.layers.13.mlp\n",
            "model.layers.13.mlp.gate_proj\n",
            "model.layers.13.mlp.up_proj\n",
            "model.layers.13.mlp.down_proj\n",
            "model.layers.13.mlp.act_fn\n",
            "model.layers.13.input_layernorm\n",
            "model.layers.13.post_attention_layernorm\n",
            "model.layers.14\n",
            "model.layers.14.self_attn\n",
            "model.layers.14.self_attn.q_proj\n",
            "model.layers.14.self_attn.k_proj\n",
            "model.layers.14.self_attn.v_proj\n",
            "model.layers.14.self_attn.o_proj\n",
            "model.layers.14.self_attn.rotary_emb\n",
            "model.layers.14.mlp\n",
            "model.layers.14.mlp.gate_proj\n",
            "model.layers.14.mlp.up_proj\n",
            "model.layers.14.mlp.down_proj\n",
            "model.layers.14.mlp.act_fn\n",
            "model.layers.14.input_layernorm\n",
            "model.layers.14.post_attention_layernorm\n",
            "model.layers.15\n",
            "model.layers.15.self_attn\n",
            "model.layers.15.self_attn.q_proj\n",
            "model.layers.15.self_attn.k_proj\n",
            "model.layers.15.self_attn.v_proj\n",
            "model.layers.15.self_attn.o_proj\n",
            "model.layers.15.self_attn.rotary_emb\n",
            "model.layers.15.mlp\n",
            "model.layers.15.mlp.gate_proj\n",
            "model.layers.15.mlp.up_proj\n",
            "model.layers.15.mlp.down_proj\n",
            "model.layers.15.mlp.act_fn\n",
            "model.layers.15.input_layernorm\n",
            "model.layers.15.post_attention_layernorm\n",
            "model.layers.16\n",
            "model.layers.16.self_attn\n",
            "model.layers.16.self_attn.q_proj\n",
            "model.layers.16.self_attn.k_proj\n",
            "model.layers.16.self_attn.v_proj\n",
            "model.layers.16.self_attn.o_proj\n",
            "model.layers.16.self_attn.rotary_emb\n",
            "model.layers.16.mlp\n",
            "model.layers.16.mlp.gate_proj\n",
            "model.layers.16.mlp.up_proj\n",
            "model.layers.16.mlp.down_proj\n",
            "model.layers.16.mlp.act_fn\n",
            "model.layers.16.input_layernorm\n",
            "model.layers.16.post_attention_layernorm\n",
            "model.layers.17\n",
            "model.layers.17.self_attn\n",
            "model.layers.17.self_attn.q_proj\n",
            "model.layers.17.self_attn.k_proj\n",
            "model.layers.17.self_attn.v_proj\n",
            "model.layers.17.self_attn.o_proj\n",
            "model.layers.17.self_attn.rotary_emb\n",
            "model.layers.17.mlp\n",
            "model.layers.17.mlp.gate_proj\n",
            "model.layers.17.mlp.up_proj\n",
            "model.layers.17.mlp.down_proj\n",
            "model.layers.17.mlp.act_fn\n",
            "model.layers.17.input_layernorm\n",
            "model.layers.17.post_attention_layernorm\n",
            "model.layers.18\n",
            "model.layers.18.self_attn\n",
            "model.layers.18.self_attn.q_proj\n",
            "model.layers.18.self_attn.k_proj\n",
            "model.layers.18.self_attn.v_proj\n",
            "model.layers.18.self_attn.o_proj\n",
            "model.layers.18.self_attn.rotary_emb\n",
            "model.layers.18.mlp\n",
            "model.layers.18.mlp.gate_proj\n",
            "model.layers.18.mlp.up_proj\n",
            "model.layers.18.mlp.down_proj\n",
            "model.layers.18.mlp.act_fn\n",
            "model.layers.18.input_layernorm\n",
            "model.layers.18.post_attention_layernorm\n",
            "model.layers.19\n",
            "model.layers.19.self_attn\n",
            "model.layers.19.self_attn.q_proj\n",
            "model.layers.19.self_attn.k_proj\n",
            "model.layers.19.self_attn.v_proj\n",
            "model.layers.19.self_attn.o_proj\n",
            "model.layers.19.self_attn.rotary_emb\n",
            "model.layers.19.mlp\n",
            "model.layers.19.mlp.gate_proj\n",
            "model.layers.19.mlp.up_proj\n",
            "model.layers.19.mlp.down_proj\n",
            "model.layers.19.mlp.act_fn\n",
            "model.layers.19.input_layernorm\n",
            "model.layers.19.post_attention_layernorm\n",
            "model.layers.20\n",
            "model.layers.20.self_attn\n",
            "model.layers.20.self_attn.q_proj\n",
            "model.layers.20.self_attn.k_proj\n",
            "model.layers.20.self_attn.v_proj\n",
            "model.layers.20.self_attn.o_proj\n",
            "model.layers.20.self_attn.rotary_emb\n",
            "model.layers.20.mlp\n",
            "model.layers.20.mlp.gate_proj\n",
            "model.layers.20.mlp.up_proj\n",
            "model.layers.20.mlp.down_proj\n",
            "model.layers.20.mlp.act_fn\n",
            "model.layers.20.input_layernorm\n",
            "model.layers.20.post_attention_layernorm\n",
            "model.layers.21\n",
            "model.layers.21.self_attn\n",
            "model.layers.21.self_attn.q_proj\n",
            "model.layers.21.self_attn.k_proj\n",
            "model.layers.21.self_attn.v_proj\n",
            "model.layers.21.self_attn.o_proj\n",
            "model.layers.21.self_attn.rotary_emb\n",
            "model.layers.21.mlp\n",
            "model.layers.21.mlp.gate_proj\n",
            "model.layers.21.mlp.up_proj\n",
            "model.layers.21.mlp.down_proj\n",
            "model.layers.21.mlp.act_fn\n",
            "model.layers.21.input_layernorm\n",
            "model.layers.21.post_attention_layernorm\n",
            "model.layers.22\n",
            "model.layers.22.self_attn\n",
            "model.layers.22.self_attn.q_proj\n",
            "model.layers.22.self_attn.k_proj\n",
            "model.layers.22.self_attn.v_proj\n",
            "model.layers.22.self_attn.o_proj\n",
            "model.layers.22.self_attn.rotary_emb\n",
            "model.layers.22.mlp\n",
            "model.layers.22.mlp.gate_proj\n",
            "model.layers.22.mlp.up_proj\n",
            "model.layers.22.mlp.down_proj\n",
            "model.layers.22.mlp.act_fn\n",
            "model.layers.22.input_layernorm\n",
            "model.layers.22.post_attention_layernorm\n",
            "model.layers.23\n",
            "model.layers.23.self_attn\n",
            "model.layers.23.self_attn.q_proj\n",
            "model.layers.23.self_attn.k_proj\n",
            "model.layers.23.self_attn.v_proj\n",
            "model.layers.23.self_attn.o_proj\n",
            "model.layers.23.self_attn.rotary_emb\n",
            "model.layers.23.mlp\n",
            "model.layers.23.mlp.gate_proj\n",
            "model.layers.23.mlp.up_proj\n",
            "model.layers.23.mlp.down_proj\n",
            "model.layers.23.mlp.act_fn\n",
            "model.layers.23.input_layernorm\n",
            "model.layers.23.post_attention_layernorm\n",
            "model.layers.24\n",
            "model.layers.24.self_attn\n",
            "model.layers.24.self_attn.q_proj\n",
            "model.layers.24.self_attn.k_proj\n",
            "model.layers.24.self_attn.v_proj\n",
            "model.layers.24.self_attn.o_proj\n",
            "model.layers.24.self_attn.rotary_emb\n",
            "model.layers.24.mlp\n",
            "model.layers.24.mlp.gate_proj\n",
            "model.layers.24.mlp.up_proj\n",
            "model.layers.24.mlp.down_proj\n",
            "model.layers.24.mlp.act_fn\n",
            "model.layers.24.input_layernorm\n",
            "model.layers.24.post_attention_layernorm\n",
            "model.layers.25\n",
            "model.layers.25.self_attn\n",
            "model.layers.25.self_attn.q_proj\n",
            "model.layers.25.self_attn.k_proj\n",
            "model.layers.25.self_attn.v_proj\n",
            "model.layers.25.self_attn.o_proj\n",
            "model.layers.25.self_attn.rotary_emb\n",
            "model.layers.25.mlp\n",
            "model.layers.25.mlp.gate_proj\n",
            "model.layers.25.mlp.up_proj\n",
            "model.layers.25.mlp.down_proj\n",
            "model.layers.25.mlp.act_fn\n",
            "model.layers.25.input_layernorm\n",
            "model.layers.25.post_attention_layernorm\n",
            "model.layers.26\n",
            "model.layers.26.self_attn\n",
            "model.layers.26.self_attn.q_proj\n",
            "model.layers.26.self_attn.k_proj\n",
            "model.layers.26.self_attn.v_proj\n",
            "model.layers.26.self_attn.o_proj\n",
            "model.layers.26.self_attn.rotary_emb\n",
            "model.layers.26.mlp\n",
            "model.layers.26.mlp.gate_proj\n",
            "model.layers.26.mlp.up_proj\n",
            "model.layers.26.mlp.down_proj\n",
            "model.layers.26.mlp.act_fn\n",
            "model.layers.26.input_layernorm\n",
            "model.layers.26.post_attention_layernorm\n",
            "model.layers.27\n",
            "model.layers.27.self_attn\n",
            "model.layers.27.self_attn.q_proj\n",
            "model.layers.27.self_attn.k_proj\n",
            "model.layers.27.self_attn.v_proj\n",
            "model.layers.27.self_attn.o_proj\n",
            "model.layers.27.self_attn.rotary_emb\n",
            "model.layers.27.mlp\n",
            "model.layers.27.mlp.gate_proj\n",
            "model.layers.27.mlp.up_proj\n",
            "model.layers.27.mlp.down_proj\n",
            "model.layers.27.mlp.act_fn\n",
            "model.layers.27.input_layernorm\n",
            "model.layers.27.post_attention_layernorm\n",
            "model.norm\n",
            "lm_head\n"
          ]
        }
      ],
      "source": [
        "def print_submodules(model, prefix=''):\n",
        "    for name, submodule in model.named_children():\n",
        "        full_name = f\"{prefix}.{name}\" if prefix else name\n",
        "        print(full_name)\n",
        "        print_submodules(submodule, full_name)\n",
        "\n",
        "# Run the function\n",
        "print_submodules(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2mlY9pMVtZH",
        "outputId": "3309c1dd-ee7d-47de-ddca-0cb3bba20e19"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Not an error, but Unsloth cannot patch MLP layers with our manual autograd engine since either LoRA adapters\n",
            "are not enabled or a bias term (like in Qwen) is used.\n",
            "Unsloth 2024.10.7 patched 28 layers with 0 QKV layers, 28 O layers and 0 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "# LoRA Adapter ì„ ì–¸\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 256, # 0ì„ ë„˜ëŠ” ìˆ«ìë¥¼ ì„ íƒí•˜ì„¸ìš”. 8, 16, 32, 64, 128ì´ ì¶”ì²œë©ë‹ˆë‹¤.\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], # target moduleë„ ì ì ˆí•˜ê²Œ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "    lora_alpha = 64,\n",
        "    lora_dropout = 0, # ì–´ë–¤ ê°’ì´ë“  ì‚¬ìš©ë  ìˆ˜ ìˆì§€ë§Œ, 0ìœ¼ë¡œ ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
        "    bias = \"none\", # ì–´ë–¤ ê°’ì´ë“  ì‚¬ìš©ë  ìˆ˜ ìˆì§€ë§Œ, \"none\"ìœ¼ë¡œ ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
        "    use_gradient_checkpointing = \"unsloth\", # ë§¤ìš° ê¸´ contextì— ëŒ€í•´ True ë˜ëŠ” \"unsloth\"ë¥¼ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.\n",
        "    random_state = 42,\n",
        "    use_rslora = True,\n",
        "    loftq_config = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loDn97x9VtZH"
      },
      "source": [
        "## 3. ë°ì´í„°ì…‹ ì „ì²˜ë¦¬\n",
        "ë°ì´í„°ì…‹ì€ í•œêµ­ì–´ ê¸ˆìœµ í•©ì„± ë°ì´í„°ì…‹ì¸ `amphora/krx-sample-instruction`ë¥¼ ì‚¬ìš©í•˜ì˜€ê³ , ë°ì´í„°ì…‹ í…œí”Œë¦¿ìœ¼ë¡œëŠ” Alpaca promptì—ì„œ inputì„ ì œê±°í•œ í˜•íƒœì˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEDXXvLTVtZH"
      },
      "outputs": [],
      "source": [
        "prompt_format = \"\"\"\n",
        "ì§€ê¸ˆë¶€í„° ë‹¹ì‹ ì€ ê¸ˆìœµê³¼ ì¬ë¬´ íšŒê³„ ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ê·œì¹™ì— ë”°ë¼ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.\n",
        "\n",
        "[í•™ìŠµ ë°ì´í„°ì˜ êµ¬ì¡°]\n",
        "1. ë¬¸ì œ ì—´: ì§ˆë¬¸ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
        "2. í•´ì„¤ ì—´: ë¬¸ì œì— ëŒ€í•œ ì„¤ëª…ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
        "3. ì •ë‹µ ì—´: ê°ê´€ì‹ ë¬¸ì œì˜ ì˜¬ë°”ë¥¸ ì •ë‹µ ë²ˆí˜¸ê°€ í¬í•¨ë˜ì–´ ìˆê±°ë‚˜, ì£¼ê´€ì‹ ë¬¸ì œì˜ ê²½ìš° ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.\n",
        "4. ë¬¸ì œ ìœ í˜• ì—´: ë¬¸ì œ ìœ í˜•ì´ 'ê°ê´€ì‹' ë˜ëŠ” 'ì£¼ê´€ì‹'ìœ¼ë¡œ ëª…ì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "[ë‹¹ì‹ ì˜ ì—­í• ]\n",
        "1. ê°ê´€ì‹ ë¬¸ì œ:\n",
        "- ì…ë ¥ëœ ì§ˆë¬¸ì„ ë¶„ì„í•©ë‹ˆë‹¤.\n",
        "- í•™ìŠµ ë°ì´í„°ì— ê¸°ë°˜í•˜ì—¬ í•´ë‹¹ ì§ˆë¬¸ì˜ ì •ë‹µ ë²ˆí˜¸ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
        "- ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸, ì„¤ëª…, í•´ì„ì€ ì¶œë ¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
        "\n",
        "2. ì£¼ê´€ì‹ ë¬¸ì œ:\n",
        "-ì…ë ¥ëœ ì§ˆë¬¸ì„ ë¶„ì„í•©ë‹ˆë‹¤.\n",
        "-í•™ìŠµ ë°ì´í„°ì— ê¸°ë°˜í•˜ì—¬ í•´ë‹¹ ì§ˆë¬¸ì˜ í•´ì„¤ ë‚´ìš©ì„ ì •ë‹µìœ¼ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
        "-í•´ì„¤ ë‚´ìš©ì„ ì •í™•íˆ ì¶œë ¥í•˜ë©° ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
        "\n",
        "--------------------------------------------------------------------\n",
        "ì•„ë˜ëŠ” ê°ê´€ì‹ ë¬¸ì œ ì…ë ¥ê³¼ ì¶œë ¥ ì˜ˆì‹œì…ë‹ˆë‹¤.\n",
        "\n",
        "##ì§ˆë¬¸ ì˜ˆì‹œ 1##\n",
        "ìµœê³ ê°€ê²©ì œì™€ ìµœì €ê°€ê²©ì œì— ëŒ€í•œ ì„¤ëª… ì¤‘ ì˜³ì€ ê²ƒì„ <ë³´ê¸°>ì—ì„œ ê³ ë¥´ë©´?\n",
        "<ë³´ê¸°>\n",
        "ã„±. ìµœê³ ê°€ê²©ì œëŠ” ê°€ê²©ì„ ì‹œì¥ ê· í˜• ê°€ê²© ì´í•˜ë¡œ í†µì œí•˜ëŠ” ì œë„ë‹¤.\n",
        "ã„´. ìµœê³ ê°€ê²©ì œì˜ ëŒ€í‘œì ì¸ ì˜ˆëŠ” ë†ì‚°ë¬¼ ê°€ê²© ì§€ì§€ì •ì±…ì´ë‹¤.\n",
        "ã„·. ì‹œì¥ì—ì„œ ìµœì €ê°€ê²©ì œë¥¼ ì‹œí–‰í•˜ë©´ ì´ˆê³¼ ê³µê¸‰ì´ ë°œìƒí•œë‹¤.\n",
        "ã„¹. ë…¸ë™ ì‹œì¥ì—ì„œ ìµœì €ê°€ê²©ì œëŠ” ìƒì‚°ìš”ì†Œì‹œì¥ì—ì„œ ì†Œë¹„ìë¥¼ ë³´í˜¸í•˜ê¸° ìœ„í•œ ì •ì±…ì´ë‹¤.\n",
        "â‘  ã„±,ã„´ â‘¡ ã„±,ã„· â‘¢ ã„´,ã„· â‘£ ã„´,ã„¹ â‘¤ ã„·,ã„¹\n",
        "##ì¶œë ¥ ì˜ˆì‹œ 1##\n",
        "â‘¡\n",
        "\n",
        "##ì§ˆë¬¸ ì˜ˆì‹œ 2##\n",
        "ì•„ë˜ ì‹ ë¬¸ê¸°ì‚¬ì˜ (A)ì— ë“¤ì–´ê°ˆ ì í•©í•œ ìš©ì–´ëŠ”?\n",
        "\"í†µê³„ì²­ì˜ <ê°€ê³„ê¸ˆìœµë³µì§€ì¡°ì‚¬>ì— ë”°ë¥´ë©´ ìµœê·¼ 3ë…„ê°„ 3040ì„¸ëŒ€ ê°€êµ¬ì˜ ë¶€ë™ì‚° ë³´ìœ ì•¡ ì¦ê°€ìœ¨ì´ ë§¤ë…„ ë‘ ìë¦¿ìˆ˜ë¥¼ ê¸°ë¡í•˜ë©° ê¸ˆìœµ ìì‚°ì¸ ì €ì¶•ì•¡ì˜ ì¦ê°€ìœ¨ì„ 2ë°° ì´ìƒ ì•ì„°ë‹¤. ì´ë“¤ ê°€êµ¬ì˜ ë¶€ë™ì‚° íˆ¬ìì˜ ì¬ì›ì€ ì–´ë””ì„œ ì˜¨ ê²ƒì¼ê¹Œ? í†µê³„ì²­ì˜ <ê°€ê³„ë™í–¥ì¡°ì‚¬>ë¡œë¶€í„° ì¬ì›ì˜ ìƒë‹¹ ë¶€ë¶„ì´ ê¸ˆìœµ ë¶€ì±„ë¡œ ì¶©ë‹¹ëìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì¬ë¬´ ê±´ì „ì„± ì§€í‘œê°€ ì§€ë‚œí•´ì—” 116.8ë¡œ ì•…í™”ëë‹¤. ì¬ë¬´ ê±´ì „ì„± ì§€í‘œê°€ 100ì„ ì›ƒëŒë©´ ì €ì¶•ì•¡ìœ¼ë¡œ ê¸ˆìœµ ë¶€ì±„ë¥¼ ì „ë¶€ ìƒí™˜í•˜ì§€ ëª»í•˜ëŠ” ìƒíƒœë¥¼ ì˜ë¯¸í•œë‹¤. ì—¬ì „íˆ (A)ë¥¼(ë¥¼) ê¸°ëŒ€í•˜ëŠ” ì‚¬ëŒë“¤ì´ ë§ë‹¤ëŠ” ëœ»ì´ë‹¤.\"\n",
        "â‘  ìºì‹œì¹´ìš° â‘¡ ì—¥ê²” ë²•ì¹™ â‘¢ í•€ë³¼ íš¨ê³¼ â‘£ íŒŒë ˆí†  ë²•ì¹™ â‘¤ ë ˆë²„ë¦¬ì§€ íš¨ê³¼\n",
        "##ì¶œë ¥ ì˜ˆì‹œ 2##\n",
        "â‘¤\n",
        "\n",
        "##ì§ˆë¬¸ ì˜ˆì‹œ 3##\n",
        "(ê°€), (ë‚˜)ì— ë“¤ì–´ê°ˆ ì•ŒíŒŒë²³ì„ ìˆœì„œëŒ€ë¡œ ì˜¬ë°”ë¥´ê²Œ ê³ ë¥¸ ê²ƒì€?\n",
        "â— ì˜¤ëœ ê²½ì œ ì¹¨ì²´ ì†ì— ì†Œë¹„ì ë¬¼ê°€ í•˜ë½ì„¸ê°€ ì ì  ì‹¬í™”í•˜ë©´ì„œ ì´ë¥¸ë°” (ê°€)ì˜ ê³µí¬ê°€ í™•ì‚°ë˜ê³  ìˆë‹¤.\n",
        "â— ë”ë¸” ë”¥(double dip)ì´ë€ ë¶ˆí™©ì—ì„œ ë²—ì–´ë‚˜ íšŒë³µ ê¸°ë¯¸ë¥¼ ë³´ì´ë˜ ê²½ê¸°ê°€ ë‹¤ì‹œ ì¹¨ì²´ì— ë¹ ì§€ëŠ” ê²ƒìœ¼ë¡œ, ë‘ ë²ˆì˜ ê²½ê¸°ì¹¨ì²´ë¥¼ ê²ªëŠ”ë‹¤ëŠ” ì ì—ì„œ (ë‚˜)ìí˜• ì¹¨ì²´ë¡œë„ ë¶ˆë¦°ë‹¤.\n",
        "â‘  D - M â‘¡ D - W â‘¢ X - M â‘£ R - M â‘¤ R - W\n",
        "##ì¶œë ¥ ì˜ˆì‹œ 3##\n",
        "â‘¡\n",
        "\n",
        "##ì§ˆë¬¸ ì˜ˆì‹œ 4##\n",
        "ê¸°ì—…ì´ ì°¨ë“±ì˜ê²°ê¶Œ, í¬ì´ì¦Œ í•„, í™©ê¸ˆë‚™í•˜ì‚° ì œë„ë¥¼ í™œìš©í•˜ëŠ” ê°€ì¥ í° ëª©ì ì€ ë¬´ì—‡ì¸ê°€?\n",
        "â‘  ë¶„ì‚°íˆ¬ì â‘¡ ê²½ì˜ê¶Œ ë°©ì–´ â‘¢ ë¬¼ë¥˜ë§ í™•ì¶© â‘£ ë…¸ë™ìƒì‚°ì„± í–¥ìƒ â‘¤ ì¸ìˆ˜Â·í•©ë³‘(M&A) ì´‰ì§„\n",
        "##ì¶œë ¥ ì˜ˆì‹œ 4##\n",
        "â‘¡\n",
        "\n",
        "--------------------------------------------------------------------\n",
        "ì•„ë˜ëŠ” ì£¼ê´€ì‹ ë¬¸ì œ ì…ë ¥ê³¼ ì¶œë ¥ ì˜ˆì‹œì…ë‹ˆë‹¤.\n",
        "\n",
        "##ì§ˆë¬¸ ì˜ˆì‹œ 1##\n",
        "Rì˜ ê³µí¬ì™€ Sì˜ ê³µí¬ë€?\n",
        "##ì¶œë ¥ ì˜ˆì‹œ 1##\n",
        "ë³´í†µ ì‹ ë¬¸ ê¸°ì‚¬ë‚˜ ë‰´ìŠ¤ì—ì„œ ì–¸ê¸‰í•˜ëŠ” Rì˜ ê³µí¬ëŠ” â€˜Recession(ê²½ê¸°ì¹¨ì²´)â€™ì„ ì˜ë¯¸í•œë‹¤. í†µìƒ ì‹œì¥ì´ â€˜ê¸°ìˆ ì  ê²½ê¸°ì¹¨ì²´â€™ë¡œ ê°„ì£¼í•˜ëŠ” ìƒí™©ì€ GDP ì¦ê°ë¥ ì´ ë‘ ë¶„ê¸° ì—°ì† ë§ˆì´ë„ˆìŠ¤ ì„±ì¥í•  ë•Œì´ë‹¤. ì €ì„±ì¥ê³¼ ê³ ë¬¼ê°€ ëŠªì— ë¹ ì§€ëŠ” ê²½ìš° â€˜Sâ€™ì˜ ê³µí¬ì— ë¹ ì¡Œë‹¤ê³  í‘œí˜„í•œë‹¤. ë³´í†µ Së¥¼ ìŠ¤íƒœê·¸í”Œë ˆì´ì…˜(Stagflation)ì„ ì˜ë¯¸í•œë‹¤. ìŠ¤íƒœê·¸í”Œë ˆì´ì…˜ì´ë€ ê²½ê¸°ê°€ ì¹¨ì²´í•´ ìˆ˜ìš”ê°€ ê°ì†Œí•¨ì—ë„ ì˜¤íˆë ¤ ë¬¼ê°€ê°€ ì˜¤ë¥´ëŠ” í˜„ìƒì´ë‹¤. ê²½ê¸°ì¹¨ì²´ì¸ ìŠ¤íƒœê·¸ë„¤ì´ì…˜(Stagnation)ê³¼ ë¬¼ê°€ ìƒìŠ¹ì„ ì˜ë¯¸í•˜ëŠ” ì¸í”Œë ˆì´ì…˜(Inflation)ì˜ í•©ì„±ì–´ì´ë‹¤.\n",
        "\n",
        "##ì§ˆë¬¸ ì˜ˆì‹œ 2##\n",
        "ì ì¬GDPê°€ ë¬´ì—‡ì¸ì§€ ì„¤ëª…í•˜ì‹œì˜¤.\n",
        "##ì¶œë ¥ ì˜ˆì‹œ 2##\n",
        "ì ì¬GDPë€ í•œ ë‚˜ë¼ì˜ ê²½ì œê°€ ì¸í”Œë ˆì´ì…˜ì„ ì¼ìœ¼í‚¤ì§€ ì•Šê³  ìƒì‚°í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ ìƒì‚°ëŸ‰ì„ ì˜ë¯¸í•˜ë©°,ê²½ì œì˜ ìƒì‚°ëŠ¥ë ¥ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì´ë‹¤. ì ì¬GDPëŠ” ë…¸ë™ë ¥,ìë³¸,ê¸°ìˆ  ë“±ì˜ ìƒì‚°ìš”ì†Œê°€ ìµœëŒ€í•œ í™œìš©í•  ë•Œ ë‹¬ì„±í•  ìˆ˜ ìˆëŠ” ìƒì‚°ëŸ‰ìœ¼ë¡œ,ê²½ì œì˜ êµ¬ì¡°ì  íŠ¹ì„±ê³¼ ê²½ì œ ì •ì±… ë“±ì— ì˜í–¥ì„ ë°›ëŠ”ë‹¤. ì ì¬GDPëŠ” ì‹¤ì œGDPì™€ ë¹„êµí•´ê²½ì œì •ì±…ì˜ ìˆ˜ë¦½ê³¼ í‰ê°€ì—ë„ ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤.ì˜ˆë¥¼ ë“¤ì–´ì •ë¶€ê°€ ê²½ì œì •ì±…ì„ ìˆ˜ë¦½í•  ë•ŒëŠ” ì ì¬GDPë¥¼ ê³ ë ¤í•˜ì—¬ ì ì ˆí•œ ì •ì±…ì„ ìˆ˜ë¦½í•´ì•¼ í•˜ë©°,ê²½ì œì˜ ì„±ì¥ë¥ ì´ ì ì¬GDPë¥¼ ì´ˆê³¼í•  ê²½ìš° ì¸í”Œë ˆì´ì…˜ì´ ë°œìƒí•  ê°€ëŠ¥ì„±ì´ ì»¤ì§€ë¯€ë¡œ,ê²½ì œì •ì±…ì„ ì¡°ì •í•˜ì—¬ ì¸í”Œë ˆì´ì…˜ì„ ì˜ˆë°©í•´ì•¼ í•œë‹¤.\n",
        "\n",
        "--------------------------------------------------------------------\n",
        "ë°˜ë“œì‹œ ì•„ë˜ ê·œì¹™ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤.\n",
        "1. ê°ê´€ì‹ ë¬¸ì œëŠ” ì •ë‹µ ë²ˆí˜¸ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
        "2. ì£¼ê´€ì‹ ë¬¸ì œëŠ” í•´ì„¤ ë‚´ìš©ì„ ì •ë‹µìœ¼ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"prompt\"]\n",
        "    outputs = examples[\"response\"]\n",
        "    texts = []\n",
        "    for instruction, output in zip(instructions, outputs):\n",
        "        text = prompt_format.format(instruction, output) + EOS_TOKEN # ë§ˆì§€ë§‰ì— eos tokenì„ ì¶”ê°€í•´ì¤Œìœ¼ë¡œì¨ ëª¨ë¸ì´ ì¶œë ¥ì„ ëë§ˆì¹  ìˆ˜ ìˆê²Œ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤.\n",
        "        texts.append(text)\n",
        "    return { \"formatted_text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"amphora/krx-sample-instructions\", split = \"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yboq7rrnFouh",
        "outputId": "fddcd9e8-597d-4d4f-ae9a-0b9f19c2a006"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'prompt': 'ìŒì•…ì‚°ì—…ì˜ ë””ì§€í„¸í™”ê°€ ì§„í–‰ë¨ì— ë”°ë¼, ì „í†µì ì¸ ìŒë°˜ ì¤‘ì‹¬ì˜ ìˆ˜ìµ ëª¨ë¸ì—ì„œ ë””ì§€í„¸ ìŒì•… ì„œë¹„ìŠ¤ ì¤‘ì‹¬ìœ¼ë¡œ ë³€í™”í–ˆìŒì„ ì„¤ëª…í•˜ëŠ”ë°, ì´ëŸ¬í•œ ë³€í™”ê°€ ì´ë£¨ì–´ì§„ ì£¼ëœ ì›ì¸ì€ ë¬´ì—‡ì´ë©°, ì´ì— ë”°ë¼ ì—…ê³„ê°€ ì–´ë–»ê²Œ ëŒ€ì‘í•˜ê³  ìˆëŠ”ì§€ êµ¬ì²´ì ì¸ ì˜ˆë¥¼ ë“¤ì–´ ì„¤ëª…í•˜ì‹œì˜¤.',\n",
              " 'response': 'ìŒì•…ì‚°ì—…ì˜ ë””ì§€í„¸í™” ì£¼ëœ ì›ì¸ì€ ì¸í„°ë„·ê³¼ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒ ì„œë¹„ìŠ¤ì˜ ë°œë‹¬ë¡œ ì¸í•´ ì†Œë¹„ìë“¤ì´ ìŒì•…ì„ ì ‘ê·¼í•˜ê³  ì†Œë¹„í•˜ëŠ” ë°©ì‹ì´ ë³€í™”í–ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë””ì§€í„¸ í”Œë«í¼ì˜ ë“±ì¥ìœ¼ë¡œ ìŒì› ìŠ¤íŠ¸ë¦¬ë° ì„œë¹„ìŠ¤(ì˜ˆ: ë©œë¡ , ìŠ¤í¬í‹°íŒŒì´ ë“±)ê°€ ë³´í¸í™”ë˜ë©´ì„œ, ì†Œë¹„ìë“¤ì€ ê³¼ê±°ì˜ ìŒë°˜ êµ¬ë§¤ ëŒ€ì‹  êµ¬ë… ê¸°ë°˜ì˜ ì„œë¹„ìŠ¤ë¥¼ í†µí•´ ìŒì•…ì— ì‰½ê²Œ ì ‘ê·¼í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.\\n\\nì´ì— ë”°ë¼ ìŒì•…ì—…ê³„ëŠ” ë‹¤ì–‘í•œ ëŒ€ì‘ ì „ëµì„ ë§ˆë ¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì•„í‹°ìŠ¤íŠ¸ë“¤ì€ ìŒì› ìŠ¤íŠ¸ë¦¬ë° ìˆ˜ìµì„ ìµœì í™”í•˜ê¸° ìœ„í•´ SNSì™€ ìœ íŠœë¸Œë¥¼ í™œìš©í•œ ë§ˆì¼€íŒ…ì„ ê°•í™”í•˜ê³  ìˆìœ¼ë©°, ë¼ì´ë¸Œ ê³µì—° ë° íŒ¬ë¯¸íŒ…ê³¼ ê°™ì€ ì˜¤í”„ë¼ì¸ ì´ë²¤íŠ¸ë¥¼ í™•ëŒ€í•˜ì—¬ ì¶”ê°€ ìˆ˜ìµì„ ì°½ì¶œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ì¼ë¶€ ìŒì•… ë ˆì´ë¸”ì€ ì•„í‹°ìŠ¤íŠ¸ì™€ì˜ í˜‘ì—…ì„ í†µí•´ ë…ì  ì½˜í…ì¸ ë¥¼ ì œì‘í•˜ê³ , ë©”íƒ€ë²„ìŠ¤ì™€ ê°™ì€ ìƒˆë¡œìš´ í”Œë«í¼ì„ ì´ìš©í•´ ê°€ìƒ ê³µì—°ì„ ê°œìµœí•˜ëŠ” ë“±ì˜ í˜ì‹ ì ì¸ ì ‘ê·¼ë²•ì„ ì‹œë„í•˜ê³  ìˆìŠµë‹ˆë‹¤.',\n",
              " '__index_level_0__': 0,\n",
              " 'formatted_text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nìŒì•…ì‚°ì—…ì˜ ë””ì§€í„¸í™”ê°€ ì§„í–‰ë¨ì— ë”°ë¼, ì „í†µì ì¸ ìŒë°˜ ì¤‘ì‹¬ì˜ ìˆ˜ìµ ëª¨ë¸ì—ì„œ ë””ì§€í„¸ ìŒì•… ì„œë¹„ìŠ¤ ì¤‘ì‹¬ìœ¼ë¡œ ë³€í™”í–ˆìŒì„ ì„¤ëª…í•˜ëŠ”ë°, ì´ëŸ¬í•œ ë³€í™”ê°€ ì´ë£¨ì–´ì§„ ì£¼ëœ ì›ì¸ì€ ë¬´ì—‡ì´ë©°, ì´ì— ë”°ë¼ ì—…ê³„ê°€ ì–´ë–»ê²Œ ëŒ€ì‘í•˜ê³  ìˆëŠ”ì§€ êµ¬ì²´ì ì¸ ì˜ˆë¥¼ ë“¤ì–´ ì„¤ëª…í•˜ì‹œì˜¤.\\n\\n### Response:\\nìŒì•…ì‚°ì—…ì˜ ë””ì§€í„¸í™” ì£¼ëœ ì›ì¸ì€ ì¸í„°ë„·ê³¼ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒ ì„œë¹„ìŠ¤ì˜ ë°œë‹¬ë¡œ ì¸í•´ ì†Œë¹„ìë“¤ì´ ìŒì•…ì„ ì ‘ê·¼í•˜ê³  ì†Œë¹„í•˜ëŠ” ë°©ì‹ì´ ë³€í™”í–ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë””ì§€í„¸ í”Œë«í¼ì˜ ë“±ì¥ìœ¼ë¡œ ìŒì› ìŠ¤íŠ¸ë¦¬ë° ì„œë¹„ìŠ¤(ì˜ˆ: ë©œë¡ , ìŠ¤í¬í‹°íŒŒì´ ë“±)ê°€ ë³´í¸í™”ë˜ë©´ì„œ, ì†Œë¹„ìë“¤ì€ ê³¼ê±°ì˜ ìŒë°˜ êµ¬ë§¤ ëŒ€ì‹  êµ¬ë… ê¸°ë°˜ì˜ ì„œë¹„ìŠ¤ë¥¼ í†µí•´ ìŒì•…ì— ì‰½ê²Œ ì ‘ê·¼í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.\\n\\nì´ì— ë”°ë¼ ìŒì•…ì—…ê³„ëŠ” ë‹¤ì–‘í•œ ëŒ€ì‘ ì „ëµì„ ë§ˆë ¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì•„í‹°ìŠ¤íŠ¸ë“¤ì€ ìŒì› ìŠ¤íŠ¸ë¦¬ë° ìˆ˜ìµì„ ìµœì í™”í•˜ê¸° ìœ„í•´ SNSì™€ ìœ íŠœë¸Œë¥¼ í™œìš©í•œ ë§ˆì¼€íŒ…ì„ ê°•í™”í•˜ê³  ìˆìœ¼ë©°, ë¼ì´ë¸Œ ê³µì—° ë° íŒ¬ë¯¸íŒ…ê³¼ ê°™ì€ ì˜¤í”„ë¼ì¸ ì´ë²¤íŠ¸ë¥¼ í™•ëŒ€í•˜ì—¬ ì¶”ê°€ ìˆ˜ìµì„ ì°½ì¶œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ì¼ë¶€ ìŒì•… ë ˆì´ë¸”ì€ ì•„í‹°ìŠ¤íŠ¸ì™€ì˜ í˜‘ì—…ì„ í†µí•´ ë…ì  ì½˜í…ì¸ ë¥¼ ì œì‘í•˜ê³ , ë©”íƒ€ë²„ìŠ¤ì™€ ê°™ì€ ìƒˆë¡œìš´ í”Œë«í¼ì„ ì´ìš©í•´ ê°€ìƒ ê³µì—°ì„ ê°œìµœí•˜ëŠ” ë“±ì˜ í˜ì‹ ì ì¸ ì ‘ê·¼ë²•ì„ ì‹œë„í•˜ê³  ìˆìŠµë‹ˆë‹¤.<|im_end|>'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNT3z9nNVtZH"
      },
      "source": [
        "## 4. ëª¨ë¸ í•™ìŠµ\n",
        "\n",
        "full trainì„ ìœ„í•´ì„œëŠ” `num_train_epochs=1`ë¡œ ì„¤ì •í•˜ê³ , `max_steps`ë¥¼ ì„¤ì •í•˜ì§€ ì•Šìœ¼ë©´ ë©ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7K5p11fVtZH",
        "outputId": "6c55656b-b4de-45d4-f742-eaa2919ae88c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"formatted_text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Trueë¡œ ì„¤ì •í•˜ë©´ ì§§ì€ í…ìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ì„œëŠ” ë” ë¹ ë¥¸ í•™ìŠµ ì†ë„ë¡œë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
        "    args = TrainingArguments( # TrainingArgumentsëŠ” ìì‹ ì˜ í•™ìŠµ í™˜ê²½ê³¼ ê¸°í˜¸ì— ë”°ë¼ ì ì ˆí•˜ê²Œ ì„¤ì •í•˜ë©´ ë©ë‹ˆë‹¤.\n",
        "        per_device_train_batch_size = 8,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 1,\n",
        "        # max_steps = 60,\n",
        "        learning_rate = 1e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 42,\n",
        "        output_dir = \"./outputs\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97c2KpwfbNmP",
        "outputId": "9f3463d4-e880-406e-bf08-f6aedba44a1c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 25,951 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 32 | Total steps = 811\n",
            " \"-____-\"     Number of trainable parameters = 161,480,704\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**** Unsloth: Please use our fixed gradient_accumulation_steps by updating transformers, TRL and Unsloth!\n",
            "`pip install --upgrade --no-cache-dir unsloth git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git`\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='85' max='811' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 85/811 25:25 < 3:42:23, 0.05 it/s, Epoch 0.10/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.241400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.110600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.090700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.033500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.065700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.044100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.070100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.015000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZl4bf7ZVtZI"
      },
      "source": [
        "## 5. ëª¨ë¸ ì¶”ë¡ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHa2tCFgVtZI"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    prompt_format.format(\n",
        "        \"ìˆ˜ìš”ì™€ ê³µê¸‰ì˜ ì›ë¦¬ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì¤˜.\", # instruction\n",
        "        \"\", # output ìƒì„±ì„ ìœ„í•´ ë¹ˆ ì¹¸ìœ¼ë¡œ ì„¤ì •\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ9S7MWeVtZI"
      },
      "source": [
        "## 6. ëª¨ë¸ ì €ì¥ ë° ì—…ë¡œë“œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DsyXPKqVtZI"
      },
      "outputs": [],
      "source": [
        "# LoRA Adapter ì €ì¥\n",
        "model.save_pretrained(\"lora_model\")\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "\n",
        "# Merged model ì €ì¥ ë° ì—…ë¡œë“œ\n",
        "model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "model.push_to_hub_merged(\"mssongit/qwen2-7b-instruct-v0\", tokenizer, save_method = \"merged_16bit\", token = \"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\") # ê°œì¸ huggingface tokenì„ ì‚¬ìš©í•˜ì—¬ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAJJR1YVLYdg"
      },
      "source": [
        "## ì°¸ê³ ìë£Œ\n",
        "\n",
        "- [Unsloth GitHub](https://github.com/unslothai/unsloth)\n",
        "- [Unsloth Docs](https://docs.unsloth.ai/)\n",
        "- [Unsloth Meta-Llama-3.1-8B Finetuning Tutorial](https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing)\n",
        "- [Huggingface PEFT](https://github.com/huggingface/peft)\n",
        "- [Huggingface TRL](https://github.com/huggingface/trl)\n",
        "- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)\n",
        "- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)\n",
        "- [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/abs/2307.08691)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LlRCGpIFoui"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXqlMGAyFoui"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ir",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}